{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://127.0.0.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------+\n",
      "|label|features                                                 |\n",
      "+-----+---------------------------------------------------------+\n",
      "|0.0  |(11,[0,1,2,4,5,6,7,10],[1.0,2.0,6.0,2.0,3.0,1.0,1.0,3.0])|\n",
      "|1.0  |(11,[0,1,3,4,7,10],[1.0,3.0,1.0,3.0,2.0,1.0])            |\n",
      "|2.0  |(11,[0,1,2,5,6,8,9],[1.0,4.0,1.0,4.0,9.0,1.0,2.0])       |\n",
      "|3.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,3.0,9.0])      |\n",
      "|4.0  |(11,[0,1,2,3,4,6,9,10],[3.0,1.0,1.0,9.0,3.0,2.0,1.0,3.0])|\n",
      "+-----+---------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The lower bound on the log likelihood of the entire corpus: -804.509976387\n",
      "The upper bound on perplexity: 3.09426915482\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|topic|termIndices|termWeights                                                    |\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|0    |[4, 7, 10] |[0.10782290096739794, 0.09748058488990814, 0.09623493832374767]|\n",
      "|1    |[9, 6, 4]  |[0.10803442582667604, 0.09660412428490929, 0.09542318257803398]|\n",
      "|2    |[1, 6, 9]  |[0.17112271358931394, 0.14580032416157745, 0.1203452124334793] |\n",
      "|3    |[1, 3, 7]  |[0.10157578994116016, 0.09974497395997303, 0.09902599170685619]|\n",
      "|4    |[3, 10, 6] |[0.23771206098568087, 0.11929728037202816, 0.094168133634538]  |\n",
      "|5    |[8, 5, 7]  |[0.10843485479690873, 0.09701498298319951, 0.09334497751854615]|\n",
      "|6    |[8, 5, 0]  |[0.09874150360081704, 0.09654274855732309, 0.09565957160673488]|\n",
      "|7    |[9, 4, 7]  |[0.11252484318647558, 0.09755091750755006, 0.09643430249601312]|\n",
      "|8    |[4, 1, 2]  |[0.109942888842421, 0.09410685582485968, 0.09374719379233712]  |\n",
      "|9    |[5, 4, 0]  |[0.15265934752869703, 0.14015418293741527, 0.13878634530351774]|\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.read.format(\"libsvm\").load(\"./test.txt\")\n",
    "\n",
    "dataset.show(5, truncate=False)\n",
    "\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(dataset)\n",
    "\n",
    "ll = model.logLikelihood(dataset)\n",
    "lp = model.logPerplexity(dataset)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(dataset)\n",
    "# transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- topic: integer (nullable = false)\n",
      " |-- termIndices: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = false)\n",
      " |-- termWeights: array (nullable = true)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/yelp.business\").load()\n",
    "review = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/yelp.review\").load()\n",
    "review = review.select('business_id', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: integer (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: integer (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(text=u\"If you need an inexpensive place to stay for a night or two then you may consider this place but for a longer stay I'd recommend somewhere with better amenities. \\n\\nPros:\\nGreat location- you're right by the train station, central location to get to old town and new town, and right by sight seeing his tours. Food, bars, and shopping all within walking distance. Location, location, location.\\nVery clean and very good maid service\\n\\nCons:\\nTiny rooms \\nUncomfortable bed \\nAbsolutely no amenities \\nNo phone in room \\nNo wardrobe \\n\\nWas given a lot of attitude about me and my husband sharing a room which was quite strange and we were charged 15 pounds more for double occupancy not sure why that matters I felt like it was a money grab. It was just handled in a kind of odd manner to me... \\n\\nIf you book this hotel all you get is a bed, desk, and a bathroom. It isn't awful but know what you're getting into.\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.select('text').rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4729\n"
     ]
    }
   ],
   "source": [
    "train_raw = review.select('text').rdd.sample(False, 0.001, 1).map(lambda x: x[0])\n",
    "print(train_raw.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"This is one star for service from Jennifer. I had set up time to see her at 9am when they opened. At 9am I was still waiting outside in line behind another gentleman, who happened to be in a wheelchair. At 9:04, Jennifer walked to the front door and unlocked it. I found it rude for her not to open the door for the wheelchair client. She hesitated to speak to me, assuming I was there with the other gentleman. If I'm going to sit in a chair with a stylist for hours, I don't care how good she/he is, I will not to pay my hard earned $ to someone who doesn't treat others kindly. I pay well for good service, so due to Jennifer, I will not only never return but I will not recommend Yelpers/friends/family to see Jennifer at Supercuts\""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction import stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = sc.textFile(\"./stop_words\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_words = ['good','great','place','time', \"n't\", 'food', 'das', 'best','will','nice','und', 'things']\n",
    "\n",
    "def tokenize(text):\n",
    "    regex = re.compile('[' + re.escape(\"!\\\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\") + '0-9\\\\r\\\\t\\\\n]')\n",
    "    text = regex.sub(\" \", text.lower())\n",
    "    words = nltk.word_tokenize(text, 'english', False)\n",
    "    words = [w for w in words if len(w) > 2 and w not in stop_words and w not in common_words]\n",
    "    return words + [words[i] + '_' + words[i+1] for i in range(len(words)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "word_occurence = train_raw.map(lambda x: [tokenize(x)]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                  _1|               words|\n",
      "+--------------------+--------------------+\n",
      "|[star, service, j...|[star, service, j...|\n",
      "|[year, son, trim,...|[year, son, trim,...|\n",
      "|[nope, happen, pr...|[nope, happen, pr...|\n",
      "|[absolute, worst,...|[absolute, worst,...|\n",
      "|[coming, years, s...|[coming, years, s...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_occurence.withColumn(\"words\", word_occurence['_1'].cast(ArrayType(StringType()))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                  _1|            features|\n",
      "+--------------------+--------------------+\n",
      "|[star, service, j...|(2000,[0,2,6,11,4...|\n",
      "|[year, son, trim,...|(2000,[15,20,43,4...|\n",
      "|[nope, happen, pr...|(2000,[83,104,163...|\n",
      "|[absolute, worst,...|(2000,[163,213,22...|\n",
      "|[coming, years, s...|(2000,[1,9,33,40,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol='_1', outputCol='features', vocabSize=2000)\n",
    "cv_model = cv.fit(word_occurence)\n",
    "train = cv_model.transform(word_occurence).cache()\n",
    "\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = LDA(k=20, maxIter=5).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[603, 708, 782, 1...|[0.01069670245101...|\n",
      "|    1|  [0, 63, 21, 2, 13]|[0.01334178064445...|\n",
      "|    2|   [0, 63, 7, 1, 33]|[0.01414439156481...|\n",
      "|    3| [29, 16, 0, 68, 26]|[0.01495260920407...|\n",
      "|    4|[33, 53, 45, 236,...|[0.01217465241637...|\n",
      "|    5|[216, 4, 48, 76, ...|[0.01565735371239...|\n",
      "|    6|[41, 119, 222, 68...|[0.01151257192206...|\n",
      "|    7|[29, 45, 216, 754...|[0.01553261415574...|\n",
      "|    8|  [3, 163, 17, 0, 4]|[0.02059190407718...|\n",
      "|    9|[147, 5, 277, 130...|[0.01495722354229...|\n",
      "|   10|   [3, 1, 37, 2, 40]|[0.01431688770436...|\n",
      "|   11|  [14, 28, 43, 6, 0]|[0.01276977619288...|\n",
      "|   12|  [9, 12, 1, 11, 28]|[0.01056777454495...|\n",
      "|   13|  [10, 25, 8, 15, 0]|[0.02250086539069...|\n",
      "|   14| [0, 27, 18, 84, 55]|[0.02194960943926...|\n",
      "|   15|    [0, 5, 1, 37, 2]|[0.01180421317801...|\n",
      "|   16|[0, 31, 138, 4, 471]|[0.01424270245321...|\n",
      "|   17|   [0, 5, 8, 22, 18]|[0.02438748123382...|\n",
      "|   18|  [0, 3, 93, 8, 319]|[0.02017931776752...|\n",
      "|   19|   [0, 27, 11, 9, 4]|[0.02680605150079...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe topics.\n",
    "topics = lda_model.describeTopics(5)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|topics_words                                      |\n",
      "+--------------------------------------------------+\n",
      "|[carne, asada, carne_asada, valley, east]         |\n",
      "|[service, bar, minutes, well, experience]         |\n",
      "|[service, bar, people, staff, years]              |\n",
      "|[pizza, restaurant, service, salad, pretty]       |\n",
      "|[years, care, family, dog, money]                 |\n",
      "|[sushi, love, happy, hour, happy_hour]            |\n",
      "|[sauce, sandwich, cream, salad, lunch]            |\n",
      "|[pizza, family, sushi, pasty, bar]                |\n",
      "|[friendly, hair, amazing, service, love]          |\n",
      "|[burger, order, pho, fries, ordered]              |\n",
      "|[friendly, staff, store, well, awesome]           |\n",
      "|[told, called, call, going, service]              |\n",
      "|[work, day, staff, recommend, called]             |\n",
      "|[chicken, delicious, ordered, definitely, service]|\n",
      "|[service, car, customer, shop, customer_service]  |\n",
      "|[service, order, staff, store, well]              |\n",
      "|[service, wait, breakfast, love, wedding]         |\n",
      "|[service, order, ordered, asked, customer]        |\n",
      "|[service, friendly, fast, ordered, chinese]       |\n",
      "|[service, car, recommend, work, love]             |\n",
      "+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Shows the result\n",
    "# transformed = lda_model.transform(train)\n",
    "# transformed.show(truncate=False)\n",
    "\n",
    "def indices_to_terms(vocabulary):\n",
    "    def indices_to_terms(xs):\n",
    "        return [vocabulary[int(x)] for x in xs]\n",
    "    return udf(indices_to_terms, ArrayType(StringType()))\n",
    "\n",
    "topics.withColumn(\"topics_words\", indices_to_terms(cv_model.vocabulary)(\"termIndices\")).select('topics_words').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         business_id|\n",
      "+--------------------+\n",
      "|qrAHt4wWRYWj1sEjx...|\n",
      "|n33Izvzk_z9_51H6N...|\n",
      "|5Ghe0btvM7tXqANnh...|\n",
      "|s_6WYPw3t50el2fSu...|\n",
      "|EvX3LA7Wc1tv1S4qr...|\n",
      "|gqv6zoGHSeTsNKynh...|\n",
      "|ynN2PeYIpufc_eSjE...|\n",
      "|g-HSA1m2vFPjKHYnq...|\n",
      "|UlDvngOADDhshzMKv...|\n",
      "|aBGT2NQ9fSesoB93-...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mesa_ids = business.filter(business.city == 'Mesa').select('business_id')\n",
    "mesa_ids.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         business_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|5Rduolg9SjUpg39hT...|This is one star ...|\n",
      "|5Rduolg9SjUpg39hT...|Took my 2 year ol...|\n",
      "|5Rduolg9SjUpg39hT...|Nope. Wont happen...|\n",
      "|5Rduolg9SjUpg39hT...|Absolute worst ha...|\n",
      "|5Rduolg9SjUpg39hT...|I have been comin...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mesa_review = mesa_ids.join(review.select('business_id', 'text'), 'business_id','inner')\n",
    "mesa_review.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_raw = mesa_review.rdd.map(lambda x: x[1]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
